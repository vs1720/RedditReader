{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\Viral Shanker\\MLfile.csv')\n",
    "data.head()\n",
    "data[\"constant\"] = 1\n",
    "data = data.drop(columns = [\"maxscore\"])\n",
    "data = data.drop(columns = [\"age\", 'created1.1', 'score1.1', 'age1.1', 'stories1.1', 'ratio1.1', 'coms1.1', 'created2.1', 'score2.1', 'age2.1', 'stories2.1', 'ratio2.1', 'coms2.1', 'created3.1', 'score3.1', 'age3.1', 'stories3.1', 'ratio3.1', 'coms3.1', 'created4.1', 'score4.1', 'age4.1', 'stories4.1', 'ratio4.1', 'coms4.1', 'created5.1', 'score5.1', 'age5.1', 'stories5.1', 'ratio5.1', 'coms5.1'])\n",
    "data[\"finalScore\"] = data[\"61selfScore\"]\n",
    "data[\"success\"] = 0\n",
    "from sklearn.utils import shuffle\n",
    "#data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(1564204038).strftime('%H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4ff9b10e9dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcurr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreshL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalScore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"success\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mcurr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#getHour and decide whether o not to shuffle\n",
    "import datetime\n",
    "def getHourIndex(pers, intercept = 37, slope = 34):\n",
    "    return(intercept + 34*pers)\n",
    "\n",
    "threshL = [2000]\n",
    "thresh = 1000\n",
    "pers = 20\n",
    "ind = getHourIndex(pers)\n",
    "\n",
    "curr = 1\n",
    "for x in threshL:\n",
    "    data.loc[data.finalScore > x, \"success\"] = curr\n",
    "    curr += 1\n",
    "\n",
    "##data.loc[data.finalScore > thresh, \"success\"] = 1\n",
    "X = data[data.columns[3:ind]]\n",
    "y = data[\"finalScore\"]\n",
    "X = pd.concat([X, data[\"created_utc\"].apply(lambda s: pd.Series({'day':datetime.datetime.utcfromtimestamp(s).strftime('%a')}))], axis=1)\n",
    "t = pd.get_dummies(X[\"day\"])\n",
    "X = X.join(t)\n",
    "X = X.drop(columns = [\"day\"])\n",
    "X = X.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"success\"]\n",
    "yr = data[\"finalScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our generalized regression testing \n",
    "\n",
    "def generalTest(reg, thresh, trainX, trainy, testX, testy):\n",
    "    reg.fit(trainX, trainy)\n",
    "    print(\"The Score is: \", reg.score(testX, testy))\n",
    "    a = np.where(testy > thresh)[0].tolist()\n",
    "    preds = reg.predict(testX)\n",
    "    p = np.where(preds > thresh)[0].tolist()\n",
    "    print(a)\n",
    "    print(p)\n",
    "    for e in p:\n",
    "        if e not in a:\n",
    "            print(\"Incorrect Prediction\", e, \"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])\n",
    "        else:\n",
    "            print(\"Correct Prediction\", e ,\"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])\n",
    "    for e in a:\n",
    "        if e not in p:\n",
    "            print(\"Missed Prediction\", e, \"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])\n",
    "\n",
    "def generalClassTest(reg, threshL, trainX, trainy, testX, testy):\n",
    "    reg.fit(trainX, trainy)\n",
    "    print(\"The Score is: \", reg.score(testX, testy))\n",
    "    for i,x in enumerate(threshL):\n",
    "        print(\"*****************************\")\n",
    "        print(\"Class\", i)\n",
    "        a = np.where(testy == i)[0].tolist()\n",
    "        preds = reg.predict(testX)\n",
    "        p = np.where(preds == i)[0].tolist()\n",
    "        print(a)\n",
    "        print(p)\n",
    "        for e in p:\n",
    "            if e not in a:\n",
    "                print(\"Incorrect Prediction\", e, \"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])\n",
    "            else:\n",
    "                print(\"Correct Prediction\", e ,\"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])\n",
    "        for e in a:\n",
    "            if e not in p:\n",
    "                print(\"Missed Prediction\", e, \"actual score:\", testy.tolist()[e], \"predicted score:\", preds.tolist()[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: x/x.max(), axis=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.apply(lambda x: x/x.max(), axis=0)\n",
    "X_train = X[:1900]\n",
    "y_train = y[:1900]\n",
    "X_test = X[1901:]\n",
    "y_test = y[1901:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "regr = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\n",
    "regrC = RandomForestClassifier(max_depth=10, random_state=0, n_estimators=100, class_weight = \"balanced\")\n",
    "regD = MLPRegressor(solver='lbfgs', hidden_layer_sizes=250,\n",
    "                           max_iter=100, shuffle=False, random_state=9876,\n",
    "                           activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Score is:  0.8719461795038883\n",
      "[27, 50, 152, 193, 376, 445, 534, 590, 660]\n",
      "[27, 50, 152, 184, 193, 206, 359, 376, 445, 590, 660]\n",
      "Correct Prediction 27 actual score: 1944 predicted score: 5087.3\n",
      "Correct Prediction 50 actual score: 7143 predicted score: 7845.47\n",
      "Correct Prediction 152 actual score: 7478 predicted score: 6654.03\n",
      "Incorrect Prediction 184 actual score: 769 predicted score: 1300.16\n",
      "Correct Prediction 193 actual score: 5967 predicted score: 7667.76\n",
      "Incorrect Prediction 206 actual score: 681 predicted score: 1146.28\n",
      "Incorrect Prediction 359 actual score: 684 predicted score: 1131.3005\n",
      "Correct Prediction 376 actual score: 4163 predicted score: 1028.65\n",
      "Correct Prediction 445 actual score: 12545 predicted score: 7712.69\n",
      "Correct Prediction 590 actual score: 2034 predicted score: 1241.9625\n",
      "Correct Prediction 660 actual score: 9136 predicted score: 7705.01\n",
      "Missed Prediction 534 actual score: 1331 predicted score: 271.79181818181814\n"
     ]
    }
   ],
   "source": [
    "generalTest(regr, thresh, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Score is:  0.07375478927203065\n",
      "*****************************\n",
      "Class 0\n",
      "[9, 29, 61, 66, 88, 99, 113, 140, 167, 215, 217, 233, 273, 287, 299, 425, 486, 521, 525, 548, 570, 584, 640, 643, 652, 655, 688, 692, 695, 718, 883, 926, 937, 939, 943, 953, 1007, 1027, 1031]\n",
      "[14, 40, 51, 66, 99, 125, 140, 154, 155, 160, 175, 215, 217, 225, 273, 277, 320, 326, 341, 391, 403, 404, 439, 484, 504, 549, 573, 584, 592, 621, 634, 637, 643, 669, 691, 718, 726, 735, 794, 805, 853, 939, 983, 1007]\n",
      "Incorrect Prediction 14 actual score: 12 predicted score: 0\n",
      "Incorrect Prediction 40 actual score: 8 predicted score: 0\n",
      "Incorrect Prediction 51 actual score: 8 predicted score: 0\n",
      "Correct Prediction 66 actual score: 0 predicted score: 0\n",
      "Correct Prediction 99 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 125 actual score: 7 predicted score: 0\n",
      "Correct Prediction 140 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 154 actual score: 5 predicted score: 0\n",
      "Incorrect Prediction 155 actual score: 8 predicted score: 0\n",
      "Incorrect Prediction 160 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 175 actual score: 1 predicted score: 0\n",
      "Correct Prediction 215 actual score: 0 predicted score: 0\n",
      "Correct Prediction 217 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 225 actual score: 5 predicted score: 0\n",
      "Correct Prediction 273 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 277 actual score: 5 predicted score: 0\n",
      "Incorrect Prediction 320 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 326 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 341 actual score: 2 predicted score: 0\n",
      "Incorrect Prediction 391 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 403 actual score: 2 predicted score: 0\n",
      "Incorrect Prediction 404 actual score: 6 predicted score: 0\n",
      "Incorrect Prediction 439 actual score: 6 predicted score: 0\n",
      "Incorrect Prediction 484 actual score: 20 predicted score: 0\n",
      "Incorrect Prediction 504 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 549 actual score: 3 predicted score: 0\n",
      "Incorrect Prediction 573 actual score: 4 predicted score: 0\n",
      "Correct Prediction 584 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 592 actual score: 6 predicted score: 0\n",
      "Incorrect Prediction 621 actual score: 3 predicted score: 0\n",
      "Incorrect Prediction 634 actual score: 2 predicted score: 0\n",
      "Incorrect Prediction 637 actual score: 2 predicted score: 0\n",
      "Correct Prediction 643 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 669 actual score: 3 predicted score: 0\n",
      "Incorrect Prediction 691 actual score: 2 predicted score: 0\n",
      "Correct Prediction 718 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 726 actual score: 1 predicted score: 0\n",
      "Incorrect Prediction 735 actual score: 2 predicted score: 0\n",
      "Incorrect Prediction 794 actual score: 3 predicted score: 0\n",
      "Incorrect Prediction 805 actual score: 11 predicted score: 0\n",
      "Incorrect Prediction 853 actual score: 3 predicted score: 0\n",
      "Correct Prediction 939 actual score: 0 predicted score: 0\n",
      "Incorrect Prediction 983 actual score: 1 predicted score: 0\n",
      "Correct Prediction 1007 actual score: 0 predicted score: 0\n",
      "Missed Prediction 9 actual score: 0 predicted score: 3\n",
      "Missed Prediction 29 actual score: 0 predicted score: 1331\n",
      "Missed Prediction 61 actual score: 0 predicted score: 15\n",
      "Missed Prediction 88 actual score: 0 predicted score: 56\n",
      "Missed Prediction 113 actual score: 0 predicted score: 1\n",
      "Missed Prediction 167 actual score: 0 predicted score: 2\n",
      "Missed Prediction 233 actual score: 0 predicted score: 6\n",
      "Missed Prediction 287 actual score: 0 predicted score: 7\n",
      "Missed Prediction 299 actual score: 0 predicted score: 5\n",
      "Missed Prediction 425 actual score: 0 predicted score: 1\n",
      "Missed Prediction 486 actual score: 0 predicted score: 10\n",
      "Missed Prediction 521 actual score: 0 predicted score: 73\n",
      "Missed Prediction 525 actual score: 0 predicted score: 2\n",
      "Missed Prediction 548 actual score: 0 predicted score: 48\n",
      "Missed Prediction 570 actual score: 0 predicted score: 3\n",
      "Missed Prediction 640 actual score: 0 predicted score: 56\n",
      "Missed Prediction 652 actual score: 0 predicted score: 1\n",
      "Missed Prediction 655 actual score: 0 predicted score: 3\n",
      "Missed Prediction 688 actual score: 0 predicted score: 2\n",
      "Missed Prediction 692 actual score: 0 predicted score: 66\n",
      "Missed Prediction 695 actual score: 0 predicted score: 3\n",
      "Missed Prediction 883 actual score: 0 predicted score: 8\n",
      "Missed Prediction 926 actual score: 0 predicted score: 2\n",
      "Missed Prediction 937 actual score: 0 predicted score: 5\n",
      "Missed Prediction 943 actual score: 0 predicted score: 1\n",
      "Missed Prediction 953 actual score: 0 predicted score: 48\n",
      "Missed Prediction 1027 actual score: 0 predicted score: 12\n",
      "Missed Prediction 1031 actual score: 0 predicted score: 10\n"
     ]
    }
   ],
   "source": [
    "generalClassTest(regrC, threshL, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = regrC.predict(X_test)\n",
    "rels = y_test\n",
    "np.where(preds == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (intro-to-ml)",
   "language": "python",
   "name": "intro-to-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
